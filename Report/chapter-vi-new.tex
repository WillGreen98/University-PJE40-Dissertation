\chapter{Implementation and Validation}

This chapter focuses on the implementation stage of this project and discusses the development of the artefact's codebase; the development of this project is dissected into its respected categories, such as each significant chunks of code and overall testing. As stated in section 1, the project will form an executable deliverable which is expected to meet the requirements outlined in \autoref{section:FunctionalRequirements} and \autoref{section:NonFunctionalRequirements} which forms the Minimum Viable Product (MVP) until the final production release. The release of the final deliverable will handled in a staged environment to be assessed.

\section{Linking to Methodology}

This project was developed in an agile manner with continuous development as seen in \autoref{fig:CRISP-DM}, it had one major iteration to which adaptations built into the existing codebase; if test features were needed, a separate git branch was created as insurance the existing (working) code was not affected. The implementation phase is broken down into two key sections as this allows the developer to assess where or not the methodology is functioning as desired with their chosen tech-stack.

\section{Elected Programming Environment}

\subsection{Development Environment}

This project has made use of popular development environments with their own intended purpose:
\begin{itemize}
    \item \textbf{\textit{PyCharm:}} for general development.
    \item \textbf{\textit{Jupyter NoteBooks:}} for data organisation and visualization.
    \item \textbf{\textit{Kaggle NoteBooks}} for open-source testing and comparison.
\end{itemize}

\subsection{Language Choice and Justification}

There are more appropriate languages to consider for this project as the nature is how differing theory implementations affect performance and accuracy; C++ was considered at the beginning of this project due to its performance and execution speeds, popular machine learning libraries are written in C++ such as TensorFlow and ported to Python with the use of Cython. However, the learning curve for writing machine learning in C++ is a lot steeper than in Python to which Python was the chosen language for ease of use and development time.

As the artefact relies on programming knowledge, it was essential to pick a language the developer is comfortable writing in as learning a language for a specific use may have required dedicating too much time, especially with time-management overhead. This was beneficial as the developer has previous experience with Python and is relatively fluent.

As stated above, many a data mining \& analytics and machine learning libraries are written for or in Python which also increases the ease of use for project development with specific traits, such as NLP with ML. This project's interests are heavily backed with open-source development of Python libraries which ensure the imports are relatively low costing and the codebases work as efficiently as possible for a given task. This makes Python a well suited language for machine learning projects and is almost the goto language for data-science related tasks. \newpage

\subsection{Language libraries and Justification}

\begin{itemize}
    \item \textbf{\textit{Numpy:}} Data-Science library.
    \item \textbf{\textit{NLTK:}} Natural Language ToolKit for Python
    \item \textbf{\textit{Pandas:}} Popular Python Library for data-science, particularly, data manipulation and formatting.
    \item \textbf{\textit{Mathplotlib:}} Popular Python library for data visualization.
\end{itemize}


\section{Data Preparation and Parsing}

Firstly, the CSV sample data file needs to be cleaned; as the sample data is relatively a small dataset, it can be cleaned manually by simply deleting the unnecessary column headers and values. This was completed within Microsoft Excel by the developer.

Within \autoref{sub:C5Preprocessing}, it states four different data preprocessing methods will be applied, the implementation of those methods are as follows, starting with the removal of punctuation and English stop words.

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{figures/chapter-6/data_preprocess.png}
    \caption[Code for punctuation and stopword removal]{Code for punctuation and stopword removal.
    \label{fig:data_preprocess}}
\end{figure}

\newpage

To remove colinear units and missing values, it is possible to use two pandas functions .drop\_duplicate() with specified parameters and .isnull() for empty items, these functions can be called when reading an object, in this case read csv. Calling these functions at the same time does not affect computational cost.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{figures/chapter-6/parse_sample.png}
    \caption[Code for colinear and missing values]{Code for colinear and missing values.
    \label{fig:parse_sample}}
\end{figure}

\subsection{Parsing Multiple Student Feedback Datasets}

The network's architecture allows for multiple data samples to be listed as sample input, this is useful as institutions may have multiple sources of training data. At the base level, a simple static file merged into a panda's dataframe will allows for all values across multiple datasets to be read simultaneously and propagated through the hidden layer. As long as the datasets are cleaned beforehand, the model will weigh and evaluative each datum entry.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{figures/chapter-6/parse_multiple_dataframes.png}
    \caption[Statically parse multiple dataset]{Statically parse multiple dataset.
    \label{fig:parse_multiple_dataframes}}
\end{figure}

\section{Network Architecture and Hyper-Parameters}

When designing the model's architecture in \autoref{sub:Word2VecSkipGram}, it was apparent the model could make use of environment variables to classify aspects of the model's training phase; the network uses seven env variables as hyper-parameters, these parameters are:

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{figures/chapter-6/network_architecture.png}
    \caption[Hyper-parameters for network]{Hyper-parameters for network.
    \label{fig:network_Hyper_parameters}}
\end{figure}

Each variable is representative of the work outlined in \autoref{sub:Word2VecSkipGram}, where:

\begin{itemize}
    \item \textbf{\textit{word dimension:}} the number of columns in the first matrix.
    \item \textbf{\textit{panel size:}} size of a vector's space.
    \item \textbf{\textit{count:}} word count in an embedding
    \item \textbf{\textit{passing epoch value:}} how many epochs the model will have, passing through nodes.
    \item \textbf{\textit{ne sample:}} negative sampling count for a word vector.
    \item \textbf{\textit{learning rate:}} variable to swap the impact of learning.
    \item \textbf{\textit{network seed:}} using a specific seed to account for reproducibility.
\end{itemize}

\section{Network Class}

As the network's environment variables have been defined, this allows a secondary use as hyper-parameters which will need to be initialised within the class name and initialisation functions.

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{figures/chapter-6/word2vecclass.png}
    \caption[Class structure for the Word2Vec SkipGram Model]{Class structure for the Word2Vec SkipGram Model.
    \label{fig:word2vec_class}}
\end{figure}

Pre-defining commonly used global variables within the class scope allows for the model to create presumptions on how the data being feed into it must be handled, specifically concerning the two matrices used as Python's array does not function as desired here to which the use of numpy's 2d array is shown.

\section{One-Hot Encoding}

As defined in \autoref{sub:Word2VecOneHotEncoding}, the One-Hot Encoding function transforms a word uni, in this case a phoneme into a column vector in cardinal space as an ordinal value, it appends the encoded value into a list as a point of reference as the vector index of the word unit itself. The encoded vectors are appended to a list which is used to calculate word unit frequency.

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{figures/chapter-6/OneHotEncoding.png}
    \caption[Coding function for encoding word units as a One-Hot vector]{Coding function for encoding word units as a One-Hot vector.
    \label{fig:onehot_encoding}}
\end{figure}

\section{Forward Propagation}

As defined in \autoref{sub:Word2VecForwardPropagation}, the forward propagation of a node inbetween the layers allows for the model to account for each node's current value against the columnn vector, now a row vector because it has been transformed due to dot multiplication between column vectors on a matric being an illegal operation, thus a row vector is required. Tranformation is handled by numpy's matrix functions and is parsed to a propagation matrix to check for current position which returns its exit node, the row vector and to what node it travelled. The exit node of the vector value is parsed via the softmax function for forward passing and to apply the chain rule.

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{figures/chapter-6/forwardpropagation.png}
    \caption[Coding function for forward propagation]{Coding function for forward propagation.
    \label{fig:forwardpropagation}}
\end{figure}

\subsection{Softmax Function}

The Softmax function is used to aid calculating error values within the current layer node, it is a standard function but is calculated differently for a partial differential as seen in this network model; the value for softmax is calculated by:

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{figures/chapter-6/softmaxfunction.png}
    \caption[Softmax function for forward error parsing]{Softmax function for forward error parsing.
    \label{fig:softmaxfunction}}
\end{figure}

\section{Backward Propagation}

As defined in \autoref{sub:Word2VecBackardPropagation}, backward propagation is the result of the partial differential of the outer layer within the model against the row vector and the current layer's error value; the embedding matrix's learning rate is multiplied by the partial differential of layer two and the context matrix is multiplied by the partial differential of layer one. The resulting values are parsed to the layer node which can account for lateral movement between layers.

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{figures/chapter-6/backwardpropagation.png}
    \caption[Coding function for backward propagation]{Coding function for backward propagation.
    \label{fig:backwardpropagation}}
\end{figure}

\section{Network Training}

Training the model requires matrix initialisation, the matrix is populated with random values around -0.1 against the vector frequency and word dimensions. The model is fundamentally orientated around an input's center word which helps for neighboring word units for surrounding word context, this is especially for student feedback because students submit entire feedback corpora at once which focuses on the distributed representation for context classification.

As stated above, column vectors are not mathematically suitable as the dot product against a matrix is illegal, another reason for transforming to a row vector is for the use of a function defined as the `row-wise function', this will summate all values in a given row for the purpose of calculating a prediction word unit.

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{figures/chapter-6/network_train.png}
    \caption[Training of network]{Training of network.
    \label{fig:network_train}}
\end{figure}

Training the model on the student feedback shows promising results with high-density matrices with corroborating values, the data shown below is one datum of student feedback per one layer node.

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{figures/chapter-6/EmbeddingMatrix.png}
    \caption[Yielded Embedding Matrix]{Yielded Embedding Matrix from Network Training.
    \label{fig:sample_emmbedding_matrix}}
\end{figure}

As with the context matrix data, the data displayed below is one datum of student feedback per one layer node.

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{figures/chapter-6/ContextMatrix.png}
    \caption[Yielded Context Matrix]{Yielded Context Matrix from Network Training.
    \label{fig:sample_context_matrix}}
\end{figure}

The code snippet displayed in \autoref{fig:network_train}, returns the following data; at the lowest level, the fundamental model used epoch values to represent the passing through the hidden layer to which calculate data loss via the loss function specified in \autoref{fig:softmaxfunction} and \autoref{eq:C5ErrorCalc}.

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{figures/chapter-6/Training_Multiple_DataSets.png}
    \caption[Training network on multiple sets of student feedback]{Training network on multiple sets of student feedback.
    \label{fig:Training_Multiple_DataSets}}
\end{figure}

The loss value for \autoref{fig:Training_Multiple_DataSets}, reached its lowest at `490.99711604131215` on epoch 138 and then proceeded to increase back to a highest of `505.5393070072687` on epoch 630. Once all epochs are passed, both matrices return their encoded column vectors as lists whereby analysis functions are executed, this allows the word2vec model to predict word units based on the training data provided, the yielded data is the vectorized numeric variation of the word unit, not the word itself.

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{figures/chapter-6/prediction_vectors.png}
    \caption[Network Prediction Vectors]{Network Prediction Vectors.
    \label{fig:prediction_vectors}}
\end{figure}

\section{Convert Word to Vector Value}

This function is how a word unit is converted to a row vector (in Python an array) which is added to the embedding matrix as part of its index value of the word value against the word vector value.

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{figures/chapter-6/convertwordvector.png}
    \caption[Conversion of word to vector value]{Conversion of word to vector value.
    \label{fig:convertwordvector}}
\end{figure}

\section{Word Weightings and Vector Sum}

\subsection{Word Vector Sum}

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{figures/chapter-6/wordvectorsum.png}
    \caption[The weighting of all word vectors]{The weighting of all word vectors.
    \label{fig:wordvectorsum}}
\end{figure}

\subsection{Word Sum}

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{figures/chapter-6/wordsum.png}
    \caption[The weighting of all words]{The weighting of all words.
    \label{fig:wordsum}}
\end{figure}

\section{Project Validation}

Any development project will have to go through several stages of testing, this could be for bugs or unexpected behaviors either during or after the design phase; machine learning projects are somewhat harder to validate as you can only validate the returned results against a given correct value that the developer knows, issues arise when trying to validate machine learning projects as requirement can often change or the training sees an unexpected spike in behaviors.

As this project did not make use of formal iterative solutions, whereby each Minimum Viable Product is an update to the previous version, it is hard to specify project validity as there was one care codebase and features were added on when needs be, this is due to not having a client and there only being one developer to organise the intended codebase.

\subsection{Graphing with Mathplotlib}

As displayed in \autoref{fig:Training_Multiple_DataSets}, it is clear the model's training is successful as with each passing epoch, the loss value decreases, finding the optimal number of epochs simply entailed increase by a factor of 1 until the loss value did not hold a constant pattren. The figures below show both of the network's matracies $W$ and $W'$ in relation to each passing epoch; with each iteration the word embeddings got higher weighted and lower in regards to word vector, whereas word context was evenly spread at a strong impact value. This shows that the model is a sucess for training student feedback and can be a viable training model for text and text classification methods.

\begin{figure}[H]
    \includegraphics[width=0.49\columnwidth]{figures/chapter-6/epoch1.png}
    \includegraphics[width=0.49\columnwidth]{figures/chapter-6/epoch58.png}
    % For long captions include a short version for the List of Figures/Tables sections in square brackets as below.
    \caption[Epoch Evolutions for Network]{Epoch Evolutions for Network, displaying the embedding matrix (green) and context matrix (red)
    \label{fig:EpochEvolutions}}
\end{figure}

Unfortunately for this project, given the data construction, there is no easy way to filter in a quantifiable medium and such categorise vector-token quality against the dataset.